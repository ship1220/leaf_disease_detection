{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyMANeohWi2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8eaf76-dec3-408d-a072-d183ade69989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4pfWbOGa6KQ",
        "outputId": "9bc8659b-b8cb-4946-dc22-0dc32105f26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Path to your zip file\n",
        "zip_path = \"/content/drive/MyDrive/plant_disease.zip\"\n",
        "extract_path = \"/content/data\"\n",
        "\n",
        "shutil.unpack_archive(zip_path, extract_path)\n",
        "print(\"Extraction done!\")\n",
        "\n",
        "# Base paths\n",
        "base_dir = \"/content/data/plant_disease_dataset\"\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "val_dir   = os.path.join(base_dir, \"val\")\n",
        "\n",
        "# Remove old train/val folders if they exist\n",
        "shutil.rmtree(train_dir, ignore_errors=True)\n",
        "shutil.rmtree(val_dir, ignore_errors=True)\n",
        "\n",
        "# Create fresh train/val folders\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX1xU3imf51y",
        "outputId": "810c6310-4bbe-4bd8-cfee-09bbb5d5544f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "source_dir = \"/content/data/PlantVillage\"  # Change if different\n",
        "split_ratio = 0.8\n",
        "\n",
        "for class_name in os.listdir(source_dir):\n",
        "    class_path = os.path.join(source_dir, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
        "        os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
        "\n",
        "        images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
        "        random.shuffle(images)\n",
        "\n",
        "        split_index = int(len(images) * split_ratio)\n",
        "        train_images = images[:split_index]\n",
        "        val_images = images[split_index:]\n",
        "\n",
        "        for img in train_images:\n",
        "            shutil.copy(os.path.join(class_path, img), os.path.join(train_dir, class_name, img))\n",
        "        for img in val_images:\n",
        "            shutil.copy(os.path.join(class_path, img), os.path.join(val_dir, class_name, img))\n",
        "\n",
        "print(\"Images successfully copied to train and val folders!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkV5OLE9gKwx",
        "outputId": "9d61ba28-7841-4de4-a608-1b258d50213d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images successfully copied to train and val folders!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_classes = set(os.listdir(train_dir))\n",
        "val_classes = set(os.listdir(val_dir))\n",
        "\n",
        "print(\"Classes in train but not in val:\", train_classes - val_classes)\n",
        "print(\"Classes in val but not in train:\", val_classes - train_classes)\n",
        "\n",
        "for cls in train_classes & val_classes:\n",
        "    train_imgs = set(os.listdir(os.path.join(train_dir, cls)))\n",
        "    val_imgs = set(os.listdir(os.path.join(val_dir, cls)))\n",
        "\n",
        "    common_imgs = train_imgs & val_imgs\n",
        "    if common_imgs:\n",
        "        print(f\"Some images are in both train and val for class '{cls}': {list(common_imgs)[:5]} ...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZDPZaIEgcwN",
        "outputId": "02c48090-ee6d-447d-8db8-3798e374fd23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes in train but not in val: set()\n",
            "Classes in val but not in train: set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Source dataset (your current dataset)\n",
        "src_dir = '/content/data/plant_disease_dataset/train'\n",
        "dst_dir = '/content/dataset_balanced/train'\n",
        "\n",
        "# Desired number of images per class\n",
        "TARGET_COUNT = 150  # You can pick 100 or 150\n",
        "\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "for class_name in os.listdir(src_dir):\n",
        "    class_path = os.path.join(src_dir, class_name)\n",
        "    if not os.path.isdir(class_path):\n",
        "        continue\n",
        "\n",
        "    images = os.listdir(class_path)\n",
        "    os.makedirs(os.path.join(dst_dir, class_name), exist_ok=True)\n",
        "\n",
        "    if len(images) > TARGET_COUNT:\n",
        "        # Undersample — randomly choose 150 images\n",
        "        selected = random.sample(images, TARGET_COUNT)\n",
        "    elif len(images) < TARGET_COUNT:\n",
        "        # Oversample — duplicate until we reach 150\n",
        "        selected = images.copy()\n",
        "        while len(selected) < TARGET_COUNT:\n",
        "            selected += random.sample(images, min(len(images), TARGET_COUNT - len(selected)))\n",
        "    else:\n",
        "        selected = images\n",
        "\n",
        "    for img in tqdm(selected, desc=f\"Copying {class_name}\"):\n",
        "        src_img = os.path.join(class_path, img)\n",
        "        dst_img = os.path.join(dst_dir, class_name, img)\n",
        "        if not os.path.exists(dst_img):\n",
        "            shutil.copy(src_img, dst_img)\n",
        "\n",
        "print(\"✅ Balanced dataset created at:\", dst_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTBmMqlDzdni",
        "outputId": "b37af7f4-0c3f-4348-ba89-5d645fcbc8f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying Tomato_Spider_mites_Two_spotted_spider_mite: 100%|██████████| 150/150 [00:00<00:00, 3647.41it/s]\n",
            "Copying Potato___Early_blight: 100%|██████████| 150/150 [00:00<00:00, 3166.80it/s]\n",
            "Copying Tomato_Early_blight: 100%|██████████| 150/150 [00:00<00:00, 4033.71it/s]\n",
            "Copying Tomato__Tomato_YellowLeaf__Curl_Virus: 100%|██████████| 150/150 [00:00<00:00, 4157.80it/s]\n",
            "Copying Pepper__bell___Bacterial_spot: 100%|██████████| 150/150 [00:00<00:00, 4618.06it/s]\n",
            "Copying Tomato_Septoria_leaf_spot: 100%|██████████| 150/150 [00:00<00:00, 4902.83it/s]\n",
            "Copying Potato___healthy: 100%|██████████| 150/150 [00:00<00:00, 4276.00it/s]\n",
            "Copying Pepper__bell___healthy: 100%|██████████| 150/150 [00:00<00:00, 2898.81it/s]\n",
            "Copying Potato___Late_blight: 100%|██████████| 150/150 [00:00<00:00, 4214.51it/s]\n",
            "Copying Tomato_Leaf_Mold: 100%|██████████| 150/150 [00:00<00:00, 4814.73it/s]\n",
            "Copying Tomato__Target_Spot: 100%|██████████| 150/150 [00:00<00:00, 4086.18it/s]\n",
            "Copying Tomato_healthy: 100%|██████████| 150/150 [00:00<00:00, 5023.24it/s]\n",
            "Copying Tomato__Tomato_mosaic_virus: 100%|██████████| 150/150 [00:00<00:00, 6898.60it/s]\n",
            "Copying Tomato_Late_blight: 100%|██████████| 150/150 [00:00<00:00, 6853.06it/s]\n",
            "Copying Tomato_Bacterial_spot: 100%|██████████| 150/150 [00:00<00:00, 6090.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Balanced dataset created at: /content/dataset_balanced/train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dst_dir,\n",
        "    target_size=(224,224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224,224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "num_classes = len(train_generator.class_indices)\n",
        "print(\"Number of classes:\", num_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrSFmiiignh8",
        "outputId": "c9cf40b2-e2d7-4731-d4d3-4a13fc7f6b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2221 images belonging to 15 classes.\n",
            "Found 4134 images belonging to 15 classes.\n",
            "Number of classes: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "\n",
        "# Freeze first layers, unfreeze last 60 layers for better fine-tuning\n",
        "for layer in base_model.layers[:-60]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[-60:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)  # dropout to reduce overfitting\n",
        "output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgpjmaRGYyvm",
        "outputId": "168aa884-05b3-4dca-e99d-4d57e53fc2e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "pm6rC2b12DzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=25,\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0geiykmRKkMS",
        "outputId": "8d0bed65-2c65-4941-94d3-232a4ac6a08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.2832 - loss: 2.3419\n",
            "Epoch 1: val_accuracy improved from -inf to 0.40445, saving model to best_model.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 6s/step - accuracy: 0.2856 - loss: 2.3345 - val_accuracy: 0.4045 - val_loss: 1.7882 - learning_rate: 1.0000e-04\n",
            "Epoch 2/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7692 - loss: 0.7772\n",
            "Epoch 2: val_accuracy did not improve from 0.40445\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 6s/step - accuracy: 0.7696 - loss: 0.7758 - val_accuracy: 0.3827 - val_loss: 1.9158 - learning_rate: 1.0000e-04\n",
            "Epoch 3/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8594 - loss: 0.4528\n",
            "Epoch 3: val_accuracy did not improve from 0.40445\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 6s/step - accuracy: 0.8596 - loss: 0.4523 - val_accuracy: 0.3667 - val_loss: 2.2549 - learning_rate: 1.0000e-04\n",
            "Epoch 4/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9022 - loss: 0.3164\n",
            "Epoch 4: val_accuracy improved from 0.40445 to 0.45936, saving model to best_model.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 6s/step - accuracy: 0.9023 - loss: 0.3162 - val_accuracy: 0.4594 - val_loss: 1.7698 - learning_rate: 1.0000e-04\n",
            "Epoch 5/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9255 - loss: 0.2467\n",
            "Epoch 5: val_accuracy improved from 0.45936 to 0.51355, saving model to best_model.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 6s/step - accuracy: 0.9255 - loss: 0.2466 - val_accuracy: 0.5135 - val_loss: 1.7008 - learning_rate: 1.0000e-04\n",
            "Epoch 6/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9508 - loss: 0.1730\n",
            "Epoch 6: val_accuracy did not improve from 0.51355\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 6s/step - accuracy: 0.9508 - loss: 0.1730 - val_accuracy: 0.4896 - val_loss: 1.9145 - learning_rate: 1.0000e-04\n",
            "Epoch 7/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9662 - loss: 0.1150\n",
            "Epoch 7: val_accuracy improved from 0.51355 to 0.57475, saving model to best_model.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 6s/step - accuracy: 0.9661 - loss: 0.1152 - val_accuracy: 0.5747 - val_loss: 1.5900 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9578 - loss: 0.1217\n",
            "Epoch 8: val_accuracy improved from 0.57475 to 0.60160, saving model to best_model.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 6s/step - accuracy: 0.9578 - loss: 0.1217 - val_accuracy: 0.6016 - val_loss: 1.4769 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9722 - loss: 0.1046\n",
            "Epoch 9: val_accuracy improved from 0.60160 to 0.64272, saving model to best_model.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 6s/step - accuracy: 0.9722 - loss: 0.1045 - val_accuracy: 0.6427 - val_loss: 1.2670 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9699 - loss: 0.0943\n",
            "Epoch 10: val_accuracy improved from 0.64272 to 0.65868, saving model to best_model.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 6s/step - accuracy: 0.9699 - loss: 0.0943 - val_accuracy: 0.6587 - val_loss: 1.2745 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9826 - loss: 0.0625\n",
            "Epoch 11: val_accuracy improved from 0.65868 to 0.75254, saving model to best_model.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 6s/step - accuracy: 0.9826 - loss: 0.0625 - val_accuracy: 0.7525 - val_loss: 0.9070 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9915 - loss: 0.0440\n",
            "Epoch 12: val_accuracy improved from 0.75254 to 0.77528, saving model to best_model.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 6s/step - accuracy: 0.9914 - loss: 0.0441 - val_accuracy: 0.7753 - val_loss: 0.7907 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9768 - loss: 0.0782\n",
            "Epoch 13: val_accuracy improved from 0.77528 to 0.79221, saving model to best_model.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 6s/step - accuracy: 0.9768 - loss: 0.0782 - val_accuracy: 0.7922 - val_loss: 0.7412 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9881 - loss: 0.0505\n",
            "Epoch 14: val_accuracy did not improve from 0.79221\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 6s/step - accuracy: 0.9881 - loss: 0.0505 - val_accuracy: 0.7274 - val_loss: 1.0347 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9854 - loss: 0.0512\n",
            "Epoch 15: val_accuracy improved from 0.79221 to 0.81809, saving model to best_model.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 6s/step - accuracy: 0.9854 - loss: 0.0513 - val_accuracy: 0.8181 - val_loss: 0.6654 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9903 - loss: 0.0435\n",
            "Epoch 16: val_accuracy improved from 0.81809 to 0.83866, saving model to best_model.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 6s/step - accuracy: 0.9903 - loss: 0.0435 - val_accuracy: 0.8387 - val_loss: 0.6102 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9846 - loss: 0.0473\n",
            "Epoch 17: val_accuracy improved from 0.83866 to 0.86913, saving model to best_model.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 6s/step - accuracy: 0.9846 - loss: 0.0473 - val_accuracy: 0.8691 - val_loss: 0.4935 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9879 - loss: 0.0400\n",
            "Epoch 18: val_accuracy did not improve from 0.86913\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 6s/step - accuracy: 0.9879 - loss: 0.0400 - val_accuracy: 0.8072 - val_loss: 0.7217 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9787 - loss: 0.0619\n",
            "Epoch 19: val_accuracy did not improve from 0.86913\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 6s/step - accuracy: 0.9787 - loss: 0.0618 - val_accuracy: 0.8113 - val_loss: 0.7073 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9911 - loss: 0.0319\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\n",
            "Epoch 20: val_accuracy did not improve from 0.86913\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 6s/step - accuracy: 0.9911 - loss: 0.0320 - val_accuracy: 0.8387 - val_loss: 0.6068 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9879 - loss: 0.0410\n",
            "Epoch 21: val_accuracy did not improve from 0.86913\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 6s/step - accuracy: 0.9879 - loss: 0.0409 - val_accuracy: 0.8396 - val_loss: 0.5772 - learning_rate: 5.0000e-05\n",
            "Epoch 22/25\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9935 - loss: 0.0260\n",
            "Epoch 22: val_accuracy did not improve from 0.86913\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 6s/step - accuracy: 0.9935 - loss: 0.0260 - val_accuracy: 0.8454 - val_loss: 0.5504 - learning_rate: 5.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/plant_disease_model1.keras\")\n"
      ],
      "metadata": {
        "id": "rXwIofe93oyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copy model\n",
        "!cp /content/best_model.keras /content/drive/MyDrive/PlantDiseaseModel/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1h2jo-jo0Zz",
        "outputId": "062eeb11-2edf-4272-ee7f-f13e512f97d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "path=\"/content/drive/MyDrive/PlantDiseaseModel/best_model.keras\"\n",
        "# -------------------------\n",
        "# Load your previously trained model\n",
        "# -------------------------\n",
        "model = load_model(path)"
      ],
      "metadata": {
        "id": "bHHv2gHqqyYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model.layers[:-60]:\n",
        "    layer.trainable = False\n",
        "for layer in model.layers[-60:]:\n",
        "    layer.trainable = True"
      ],
      "metadata": {
        "id": "1T0M4slmrV54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),  # smaller LR to preserve learned weights\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ],
      "metadata": {
        "id": "xbcnOxLCrYkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "rXBwNHikrbWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model_continued.keras',  # save to new file to preserve old model\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Continue training\n",
        "# -------------------------\n",
        "history = model.fit(\n",
        "    train_generator,           # your training data generator\n",
        "    validation_data=val_generator,  # your validation data generator\n",
        "    epochs=15,                 # fewer epochs for fine-tuning\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z16jiRqrrf19",
        "outputId": "a361382a-9937-4a85-ae8f-89d4bbc51132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638ms/step - accuracy: 0.9246 - loss: 0.2513\n",
            "Epoch 1: val_accuracy improved from -inf to 0.89115, saving model to best_model_continued.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 972ms/step - accuracy: 0.9246 - loss: 0.2512 - val_accuracy: 0.8911 - val_loss: 0.3842 - learning_rate: 1.0000e-05\n",
            "Epoch 2/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step - accuracy: 0.9364 - loss: 0.1947\n",
            "Epoch 2: val_accuracy improved from 0.89115 to 0.90252, saving model to best_model_continued.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 480ms/step - accuracy: 0.9364 - loss: 0.1948 - val_accuracy: 0.9025 - val_loss: 0.3405 - learning_rate: 1.0000e-05\n",
            "Epoch 3/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - accuracy: 0.9518 - loss: 0.1839\n",
            "Epoch 3: val_accuracy improved from 0.90252 to 0.90687, saving model to best_model_continued.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 489ms/step - accuracy: 0.9518 - loss: 0.1841 - val_accuracy: 0.9069 - val_loss: 0.3193 - learning_rate: 1.0000e-05\n",
            "Epoch 4/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385ms/step - accuracy: 0.9518 - loss: 0.1725\n",
            "Epoch 4: val_accuracy improved from 0.90687 to 0.91485, saving model to best_model_continued.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 477ms/step - accuracy: 0.9518 - loss: 0.1724 - val_accuracy: 0.9149 - val_loss: 0.2922 - learning_rate: 1.0000e-05\n",
            "Epoch 5/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385ms/step - accuracy: 0.9471 - loss: 0.1719\n",
            "Epoch 5: val_accuracy improved from 0.91485 to 0.92066, saving model to best_model_continued.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 505ms/step - accuracy: 0.9472 - loss: 0.1718 - val_accuracy: 0.9207 - val_loss: 0.2695 - learning_rate: 1.0000e-05\n",
            "Epoch 6/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - accuracy: 0.9586 - loss: 0.1354\n",
            "Epoch 6: val_accuracy improved from 0.92066 to 0.92840, saving model to best_model_continued.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 480ms/step - accuracy: 0.9586 - loss: 0.1356 - val_accuracy: 0.9284 - val_loss: 0.2502 - learning_rate: 1.0000e-05\n",
            "Epoch 7/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step - accuracy: 0.9659 - loss: 0.1209\n",
            "Epoch 7: val_accuracy improved from 0.92840 to 0.93372, saving model to best_model_continued.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 489ms/step - accuracy: 0.9659 - loss: 0.1208 - val_accuracy: 0.9337 - val_loss: 0.2251 - learning_rate: 1.0000e-05\n",
            "Epoch 8/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - accuracy: 0.9580 - loss: 0.1195\n",
            "Epoch 8: val_accuracy improved from 0.93372 to 0.93880, saving model to best_model_continued.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 563ms/step - accuracy: 0.9580 - loss: 0.1195 - val_accuracy: 0.9388 - val_loss: 0.2038 - learning_rate: 1.0000e-05\n",
            "Epoch 9/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step - accuracy: 0.9600 - loss: 0.1174\n",
            "Epoch 9: val_accuracy improved from 0.93880 to 0.94098, saving model to best_model_continued.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 544ms/step - accuracy: 0.9600 - loss: 0.1175 - val_accuracy: 0.9410 - val_loss: 0.2024 - learning_rate: 1.0000e-05\n",
            "Epoch 10/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - accuracy: 0.9627 - loss: 0.1133\n",
            "Epoch 10: val_accuracy improved from 0.94098 to 0.94291, saving model to best_model_continued.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 481ms/step - accuracy: 0.9626 - loss: 0.1133 - val_accuracy: 0.9429 - val_loss: 0.1912 - learning_rate: 1.0000e-05\n",
            "Epoch 11/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - accuracy: 0.9625 - loss: 0.1078\n",
            "Epoch 11: val_accuracy improved from 0.94291 to 0.94315, saving model to best_model_continued.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 547ms/step - accuracy: 0.9625 - loss: 0.1078 - val_accuracy: 0.9432 - val_loss: 0.1868 - learning_rate: 1.0000e-05\n",
            "Epoch 12/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - accuracy: 0.9650 - loss: 0.1277\n",
            "Epoch 12: val_accuracy improved from 0.94315 to 0.94461, saving model to best_model_continued.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 499ms/step - accuracy: 0.9650 - loss: 0.1275 - val_accuracy: 0.9446 - val_loss: 0.1729 - learning_rate: 1.0000e-05\n",
            "Epoch 13/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - accuracy: 0.9730 - loss: 0.0954\n",
            "Epoch 13: val_accuracy improved from 0.94461 to 0.94727, saving model to best_model_continued.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 495ms/step - accuracy: 0.9730 - loss: 0.0955 - val_accuracy: 0.9473 - val_loss: 0.1659 - learning_rate: 1.0000e-05\n",
            "Epoch 14/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step - accuracy: 0.9718 - loss: 0.0957\n",
            "Epoch 14: val_accuracy improved from 0.94727 to 0.95065, saving model to best_model_continued.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 516ms/step - accuracy: 0.9718 - loss: 0.0958 - val_accuracy: 0.9507 - val_loss: 0.1614 - learning_rate: 1.0000e-05\n",
            "Epoch 15/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - accuracy: 0.9761 - loss: 0.0834\n",
            "Epoch 15: val_accuracy did not improve from 0.95065\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 491ms/step - accuracy: 0.9761 - loss: 0.0834 - val_accuracy: 0.9507 - val_loss: 0.1613 - learning_rate: 1.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "path=\"/content/drive/MyDrive/best_model_continued.keras\"\n",
        "# -------------------------\n",
        "# Load the previously trained model\n",
        "# -------------------------\n",
        "model = load_model(path)  # Replace with your saved model filename\n",
        "\n",
        "# -------------------------\n",
        "# Unfreeze last 60 layers for fine-tuning\n",
        "# -------------------------\n",
        "for layer in model.layers[:-60]:\n",
        "    layer.trainable = False\n",
        "for layer in model.layers[-60:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# -------------------------\n",
        "# Recompile with smaller learning rate\n",
        "# -------------------------\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),  # small LR for fine-tuning\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Callbacks (no continuous saving to speed up training)\n",
        "# -------------------------\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True  # ensures the model ends up with best weights\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Continue training\n",
        "# -------------------------\n",
        "history = model.fit(\n",
        "    train_generator,            # your training data\n",
        "    validation_data=val_generator,  # your validation data\n",
        "    epochs=15,                  # fewer epochs since model is already trained\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_nscDDHx-6L",
        "outputId": "fec72a22-426c-47bf-960b-7014f99c4cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 929ms/step - accuracy: 0.9522 - loss: 0.1639 - val_accuracy: 0.9475 - val_loss: 0.1639 - learning_rate: 1.0000e-05\n",
            "Epoch 2/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 476ms/step - accuracy: 0.9457 - loss: 0.1452 - val_accuracy: 0.9465 - val_loss: 0.1679 - learning_rate: 1.0000e-05\n",
            "Epoch 3/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 450ms/step - accuracy: 0.9557 - loss: 0.1216 - val_accuracy: 0.9465 - val_loss: 0.1628 - learning_rate: 1.0000e-05\n",
            "Epoch 4/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 468ms/step - accuracy: 0.9582 - loss: 0.1130 - val_accuracy: 0.9468 - val_loss: 0.1604 - learning_rate: 1.0000e-05\n",
            "Epoch 5/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 457ms/step - accuracy: 0.9511 - loss: 0.1334 - val_accuracy: 0.9458 - val_loss: 0.1602 - learning_rate: 1.0000e-05\n",
            "Epoch 6/15\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 467ms/step - accuracy: 0.9609 - loss: 0.1215 - val_accuracy: 0.9470 - val_loss: 0.1571 - learning_rate: 1.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"plant_disease_model_finetuned.keras\")\n"
      ],
      "metadata": {
        "id": "DMDgiEGy4liS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "path=\"/content/drive/MyDrive/plant_disease_model_finetuned.keras\"\n",
        "#  Load previously trained/best model\n",
        "model = load_model(path)\n",
        "\n",
        "#  Unfreeze more layers to allow deeper fine-tuning\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True  # Unfreeze all layers (you can choose partial if needed)\n",
        "\n",
        "#  Recompile after changing trainability\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),  # lower LR for fine-tuning\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks for safety\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=7,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'plant_disease_model_finetuned.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Continue training\n",
        "history2 = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20,  # increase if you want deeper training\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#  Save final model\n",
        "model.save(\"plant_disease_model_finetuned.keras\")\n",
        "print(\"Model saved successfully as plant_disease_model_finetuned.keras\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgnmA7HB5_11",
        "outputId": "9a42bcb0-af6d-423f-9764-5add8c0a94fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588ms/step - accuracy: 0.9077 - loss: 0.3059\n",
            "Epoch 1: val_accuracy improved from -inf to 0.94678, saving model to plant_disease_model_finetuned.keras\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 705ms/step - accuracy: 0.9077 - loss: 0.3057 - val_accuracy: 0.9468 - val_loss: 0.1701 - learning_rate: 1.0000e-05\n",
            "Epoch 2/20\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - accuracy: 0.9303 - loss: 0.2231\n",
            "Epoch 2: val_accuracy did not improve from 0.94678\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 439ms/step - accuracy: 0.9302 - loss: 0.2231 - val_accuracy: 0.9352 - val_loss: 0.1948 - learning_rate: 1.0000e-05\n",
            "Epoch 3/20\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - accuracy: 0.9359 - loss: 0.1867\n",
            "Epoch 3: val_accuracy did not improve from 0.94678\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 440ms/step - accuracy: 0.9359 - loss: 0.1868 - val_accuracy: 0.9323 - val_loss: 0.2222 - learning_rate: 1.0000e-05\n",
            "Epoch 4/20\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - accuracy: 0.9432 - loss: 0.1793\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\n",
            "Epoch 4: val_accuracy did not improve from 0.94678\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 439ms/step - accuracy: 0.9432 - loss: 0.1793 - val_accuracy: 0.9209 - val_loss: 0.2564 - learning_rate: 1.0000e-05\n",
            "Epoch 5/20\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step - accuracy: 0.9582 - loss: 0.1427\n",
            "Epoch 5: val_accuracy did not improve from 0.94678\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 460ms/step - accuracy: 0.9582 - loss: 0.1428 - val_accuracy: 0.9124 - val_loss: 0.2854 - learning_rate: 5.0000e-06\n",
            "Epoch 6/20\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - accuracy: 0.9552 - loss: 0.1512\n",
            "Epoch 6: val_accuracy did not improve from 0.94678\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 437ms/step - accuracy: 0.9551 - loss: 0.1512 - val_accuracy: 0.9047 - val_loss: 0.3072 - learning_rate: 5.0000e-06\n",
            "Epoch 7/20\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step - accuracy: 0.9480 - loss: 0.1475\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
            "\n",
            "Epoch 7: val_accuracy did not improve from 0.94678\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 438ms/step - accuracy: 0.9480 - loss: 0.1475 - val_accuracy: 0.9040 - val_loss: 0.3160 - learning_rate: 5.0000e-06\n",
            "Epoch 8/20\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - accuracy: 0.9578 - loss: 0.1344\n",
            "Epoch 8: val_accuracy did not improve from 0.94678\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 440ms/step - accuracy: 0.9578 - loss: 0.1344 - val_accuracy: 0.8982 - val_loss: 0.3321 - learning_rate: 2.5000e-06\n",
            "✅ Model saved successfully as plant_disease_model_finetuned.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "path=\"/content/drive/MyDrive/plant_disease_model_finetuned.keras\"\n",
        "# Load previously trained/best model\n",
        "model = load_model(path)\n",
        "\n",
        "# Unfreeze more layers to allow deeper fine-tuning\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True  # Unfreeze all layers (you can choose partial if needed)\n",
        "\n",
        "# Recompile after changing trainability\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),  # lower LR for fine-tuning\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks for safety\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=7,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'plant_disease_model_finetuned.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Continue training\n",
        "history2 = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20,  # increase if you want deeper training\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#  Save final model\n",
        "model.save(\"plant_disease_model_finetuned.keras\")\n",
        "print(\"Model saved successfully as plant_disease_model_finetuned.keras\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L4t5utMETRb",
        "outputId": "c749fcdf-8abd-431a-ee92-bd53fda21b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.9108 - loss: 0.3025\n",
            "Epoch 1: val_accuracy improved from -inf to 0.94388, saving model to plant_disease_model_finetuned.keras\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 9s/step - accuracy: 0.9110 - loss: 0.3017 - val_accuracy: 0.9439 - val_loss: 0.1837 - learning_rate: 1.0000e-05\n",
            "Epoch 2/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.9360 - loss: 0.2023\n",
            "Epoch 2: val_accuracy did not improve from 0.94388\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 9s/step - accuracy: 0.9360 - loss: 0.2024 - val_accuracy: 0.9342 - val_loss: 0.2066 - learning_rate: 1.0000e-05\n",
            "Epoch 3/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.9285 - loss: 0.2126\n",
            "Epoch 3: val_accuracy did not improve from 0.94388\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 9s/step - accuracy: 0.9286 - loss: 0.2126 - val_accuracy: 0.9313 - val_loss: 0.2229 - learning_rate: 1.0000e-05\n",
            "Epoch 4/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.9487 - loss: 0.1603\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\n",
            "Epoch 4: val_accuracy did not improve from 0.94388\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 9s/step - accuracy: 0.9487 - loss: 0.1605 - val_accuracy: 0.9255 - val_loss: 0.2406 - learning_rate: 1.0000e-05\n",
            "Epoch 5/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.9582 - loss: 0.1444\n",
            "Epoch 5: val_accuracy did not improve from 0.94388\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 9s/step - accuracy: 0.9581 - loss: 0.1446 - val_accuracy: 0.9204 - val_loss: 0.2612 - learning_rate: 5.0000e-06\n",
            "Epoch 6/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.9401 - loss: 0.1711\n",
            "Epoch 6: val_accuracy did not improve from 0.94388\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m674s\u001b[0m 10s/step - accuracy: 0.9402 - loss: 0.1709 - val_accuracy: 0.9139 - val_loss: 0.2823 - learning_rate: 5.0000e-06\n",
            "Epoch 7/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.9486 - loss: 0.1605\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
            "\n",
            "Epoch 7: val_accuracy did not improve from 0.94388\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 9s/step - accuracy: 0.9486 - loss: 0.1604 - val_accuracy: 0.9093 - val_loss: 0.2986 - learning_rate: 5.0000e-06\n",
            "Epoch 8/20\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.9494 - loss: 0.1650\n",
            "Epoch 8: val_accuracy did not improve from 0.94388\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 9s/step - accuracy: 0.9495 - loss: 0.1650 - val_accuracy: 0.9069 - val_loss: 0.3091 - learning_rate: 2.5000e-06\n",
            "✅ Model saved successfully as plant_disease_model_finetuned.keras\n"
          ]
        }
      ]
    }
  ]
}